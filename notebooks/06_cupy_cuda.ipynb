{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CuPy Intro\n",
        "* CuPy is an open-source library for easy GPU-accelerated computing in Python.\n",
        "* Makes use of the most modern and optimized CUDA libraries.\n",
        "* Highly compatible with Numpy, Scipy and other popular packages.\n",
        "* Easy to install, supports NVIDIA and AMD GPUs.\n",
        "* Provides alternative ways to express GPU kernels. "
      ],
      "metadata": {
        "id": "W-SlYMAL_mGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the available device and its attributes"
      ],
      "metadata": {
        "id": "sMH4mkZucCmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "device = cp.cuda.Device()\n",
        "device.use()\n",
        "\n",
        "print('Using device: ', cp.cuda.runtime.getDeviceProperties(device)['name'])\n",
        "\n",
        "attributes = device.attributes\n",
        "properties = cp.cuda.runtime.getDeviceProperties(device)\n",
        "print('Number of SMs: ', attributes['MultiProcessorCount'])\n",
        "print('Maximum threads per Block: ', properties['maxThreadsPerBlock'])\n",
        "print('Shared memory size (KB): ', properties['sharedMemPerBlock']/1024)\n",
        "print('Global memory size (GB): ', properties['totalGlobalMem'] / (1024**3))\n"
      ],
      "metadata": {
        "id": "gcYr0izT-zsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that:\n",
        "* A GPU has typically more cores (x10) than a CPU. \n",
        "* The fast shared memory is 6 orders of magnitude smaller than the global memory. "
      ],
      "metadata": {
        "id": "zXmSnT-FbRgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CuPy Arrays\n",
        "Almost identical interface with Numpy arrays. \n"
      ],
      "metadata": {
        "id": "LXG_IPR_cC1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Supports all array creation routines, like zeros, ones, empty, etc\n",
        "dev_a = cp.arange(10, dtype=int)\n",
        "dev_b = cp.array([1, 2, 3, 4])\n",
        "print(type(dev_a))\n",
        "\n",
        "# Can be printed out of the box, though this results in device-host memory copying \n",
        "%time print(dev_a)\n",
        "\n",
        "a = np.arange(10, dtype=int)\n",
        "%time print(a)"
      ],
      "metadata": {
        "id": "UWdP9Q3Wc8RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cupy also supports all sorts of fancy indexing\n",
        "\n",
        "# strided with start stop index\n",
        "print(dev_a[1:-1:2])\n",
        "# using list of indices to gather\n",
        "print(dev_a[[0,2,4]])\n",
        "# or with boolean list\n",
        "print(dev_a[dev_a % 3 == 0])"
      ],
      "metadata": {
        "id": "mOY8Vbyql3tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Easy to transfer arrays between the device and the host\n",
        "\n",
        "a = np.arange(0, 20, 2)\n",
        "dev_a = cp.asarray(a)\n",
        "\n",
        "# The two arrays contain the same elements\n",
        "print(np.allclose(a, dev_a))\n",
        "print(cp.allclose(a, dev_a))\n",
        "# Notice that (many) numpy and cupy functions can accept as arguments both numpy and cupy arrays!"
      ],
      "metadata": {
        "id": "y36fXnRBjRhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get an array back to the host is simple:\n",
        "b = cp.asnumpy(dev_a)\n",
        "c = dev_a.get()\n",
        "print(type(b), type(c))\n",
        "\n",
        "# Cupy can  actually operate solely on numpy arrays\n",
        "print(cp.allclose(b, c))\n"
      ],
      "metadata": {
        "id": "RixjN8LulzlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supported functions\n",
        "\n",
        "Complete list here:  https://docs.cupy.dev/en/stable/reference/comparison.html\n",
        "\n",
        "Includes Numpy and Scipy routines. \n",
        "\n",
        "CuPy behaves like a drop-in replacement of Numpy:"
      ],
      "metadata": {
        "id": "4hs39nAecC8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "for xp in [np, cp]:\n",
        "    x = xp.arange(10)\n",
        "    W = xp.ones((10, 5))\n",
        "    y = xp.dot(x, W)\n",
        "    print(y)"
      ],
      "metadata": {
        "id": "ARdY5w5NedfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ways to GPU-accelerate a function: Using Numpy/ scipy equivalent operations\n",
        "The easiest way is by using numpy-like array operations and the supported functions."
      ],
      "metadata": {
        "id": "fuuw3JzILxJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It is trickier to time GPU kernels, because they behave asynchronously w.r.t the host\n",
        "def benchmark(func, args, n_repeat=10, n_warmup=1):\n",
        "    import time \n",
        "    import cupy as cp\n",
        "    start_gpu = cp.cuda.Event()\n",
        "    end_gpu = cp.cuda.Event()\n",
        "    for i in range(n_warmup):\n",
        "        out = func(*args)\n",
        "\n",
        "    start_gpu.record()\n",
        "    start_cpu = time.perf_counter()\n",
        "    for i in range(n_repeat):\n",
        "        out = func(*args)\n",
        "\n",
        "    end_cpu = time.perf_counter()\n",
        "    end_gpu.record()\n",
        "    end_gpu.synchronize()\n",
        "    t_gpu = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
        "    t_cpu = 1000 * (end_cpu - start_cpu)\n",
        "    print('Average GPU time (ms): ', t_gpu / n_repeat)\n",
        "    print('Average CPU time (ms): ', t_cpu/ n_repeat)\n"
      ],
      "metadata": {
        "id": "GfplqvDF0Rc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saxpy_trig, just a random compute intensive function\n",
        "def saxpy_trig(x, y, a):\n",
        "    return cp.exp(a * cp.sin(x) + cp.cos(y))\n",
        "\n",
        "dev_x = cp.random.uniform(size=10000000, dtype=np.float32)\n",
        "dev_y = cp.random.uniform(size=10000000, dtype=np.float32)\n",
        "a = 0.5"
      ],
      "metadata": {
        "id": "263zDgmfjIzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(saxpy_trig, (dev_x, dev_y, a))"
      ],
      "metadata": {
        "id": "xfH393xkuJwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ways to GPU-accelerate a function: 2) User defined, elementwise or reduction kernels"
      ],
      "metadata": {
        "id": "W6txU784cC-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elementwise kernels\n",
        "When you want to compute an operation that operates on corresponding elements within the input arrays, e.g. `arr_a + arr_b` \n",
        "```python\n",
        "for i in size:\n",
        "    y[i] = F(a[i], b[i], ..., c1, c2, ...)\n",
        "```"
      ],
      "metadata": {
        "id": "O_Icg60Xcf_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saxpy_trig_elemwise = cp.ElementwiseKernel(\n",
        "    'float32 x, float32 y, float32 a',  # Input types\n",
        "    'float32 z',                        # Output type\n",
        "    'z = exp(a * sin(x) + cos(y))',     # operation\n",
        "    'saxpy_trig_elemwise'               # Kernel name\n",
        ")\n"
      ],
      "metadata": {
        "id": "1wgnV2f8uoYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(saxpy_trig_elemwise, (dev_x, dev_y, a))"
      ],
      "metadata": {
        "id": "W0YD9a0lvTJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduction kernels\n",
        "When you want to reduce an array to a single element, e.g. `arr_a.sum(), arr_a.max()`.\n",
        "```python\n",
        "y = c\n",
        "for i in size:\n",
        "    y = F(y, a[i], b[i], ..., c1, c2, ...)\n",
        "```"
      ],
      "metadata": {
        "id": "VAm9hy_VciCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saxpy_trig_sum_reduction = cp.ReductionKernel(\n",
        "    'float32 x, float32 y, float32 a',      # input arguments\n",
        "    'float32 z',                            # output arguments\n",
        "    'exp(a * sin(x) + cos(y))',             # map expression\n",
        "    'a + b',                                # Reduce expression\n",
        "    'z = a',                                # post map expression\n",
        "    '0',                                    # identity element\n",
        "    'saxpy_trig_sum_reduction'              # name\n",
        ")"
      ],
      "metadata": {
        "id": "9aKKCk_T2qYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(saxpy_trig_sum_reduction, (dev_x, dev_y, a))"
      ],
      "metadata": {
        "id": "rjH4od8K3lED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ways to GPU-accelerate a function: 3) Kernel fusion\n",
        "Fuses together multiple operations in a single kernel launch."
      ],
      "metadata": {
        "id": "FzQtOZ16cDBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cp.fuse(kernel_name='saxpy_trig_fused')\n",
        "def saxpy_trig_fused(x, y, a):\n",
        "    return cp.exp(a * cp.sin(x) + cp.cos(y))\n"
      ],
      "metadata": {
        "id": "xyfHl3nkO5p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(saxpy_trig_fused, (dev_x, dev_y, a))"
      ],
      "metadata": {
        "id": "E403Z3xBPI8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interoperability\n",
        "CuPy can be combined with a bunch of other libraries, including Numpy, mpi4py, Pytorch and ... Numba!"
      ],
      "metadata": {
        "id": "yZQsSeuWcDEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import vectorize, cuda\n",
        "import math\n",
        "\n",
        "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
        "def saxpy_trig_numba_vec(x, y, a):\n",
        "    return math.exp(a * math.sin(x) + math.cos(y))"
      ],
      "metadata": {
        "id": "bZL_-3ZMSaOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(saxpy_trig_numba_vec, (dev_x, dev_y, a))"
      ],
      "metadata": {
        "id": "oRUrdfA7Titk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def saxpy_trig_numba_jit(x, y, a, out):\n",
        "    tid = cuda.grid(1)\n",
        "    if tid < x.shape[0]:\n",
        "        out[tid] = math.exp(a * math.sin(x[tid]) + math.cos(y[tid]))\n",
        "\n",
        "dev_out = cuda.device_array(len(dev_x), dtype=np.float32)\n",
        "\n",
        "block_size = 1024\n",
        "grid_size = int((len(dev_x) + block_size-1)// block_size)\n"
      ],
      "metadata": {
        "id": "gjSsfOtETCoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark(saxpy_trig_numba_jit[grid_size, block_size], (dev_x, dev_y, a, dev_out))"
      ],
      "metadata": {
        "id": "5kmc89eqV0Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that in the previous function calls we passed cupy arrays to Numba CUDA jitted functions, and even a mix of Cupy + Numba cuda arrays. "
      ],
      "metadata": {
        "id": "0P9eu_N-ZLU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(dev_out))\n",
        "# Zero-copy conversions \n",
        "dev_cp_out = cp.asarray(dev_out)\n",
        "print(type(dev_cp_out))"
      ],
      "metadata": {
        "id": "RHbq3cymY3tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other interesting features of CuPy include\n",
        "* JIT decorator to compile unrolled, python-like kernels to CUDA kernels\n",
        "* Can easily import + compile raw CUDA source files. \n",
        "* Acceleration libraries: Some of the most basic operations are accelerated using HPC CUDA backend like CUB and CuTENSOR.\n",
        "* AMD support: CuPy has experimental support for AMD GPUs (ROCm). Increasing set of features supported in AMD GPUs. "
      ],
      "metadata": {
        "id": "4K8vLY8Ojtq4"
      }
    }
  ]
}