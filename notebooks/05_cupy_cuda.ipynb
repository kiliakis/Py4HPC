{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-SlYMAL_mGw"
   },
   "source": [
    "# CuPy Intro\n",
    "* CuPy is an open-source library for easy GPU-accelerated computing in Python.\n",
    "* Makes use of the most modern and optimized CUDA libraries.\n",
    "* Extremely low-overhead.\n",
    "* Highly compatible with Numpy, Scipy and other popular packages.\n",
    "* Easy to install, supports NVIDIA and AMD GPUs.\n",
    "* Provides multiple ways to express GPU kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMH4mkZucCmy"
   },
   "source": [
    "# Exploring the available device and its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcYr0izT-zsH"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "device = cp.cuda.Device()\n",
    "device.use()\n",
    "\n",
    "print('Using device: ', cp.cuda.runtime.getDeviceProperties(device)['name'])\n",
    "\n",
    "attributes = device.attributes\n",
    "properties = cp.cuda.runtime.getDeviceProperties(device)\n",
    "print('Number of SMs: ', attributes['MultiProcessorCount'])\n",
    "print('Maximum threads per Block: ', properties['maxThreadsPerBlock'])\n",
    "print('Shared memory size (KB): ', properties['sharedMemPerBlock']/1024)\n",
    "print('Global memory size (GB): ', properties['totalGlobalMem'] / (1024**3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXmSnT-FbRgM"
   },
   "source": [
    "Notice that:\n",
    "* A GPU has typically more cores (x10) than a CPU. \n",
    "* The shared memory is $10^6$ than the global memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXG_IPR_cC1d"
   },
   "source": [
    "# CuPy Arrays\n",
    "Almost identical interface with Numpy arrays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWdP9Q3Wc8RH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Supports all array creation routines, like zeros, ones, empty, etc\n",
    "dev_a = cp.arange(10, dtype=int)\n",
    "dev_b = cp.array([1, 2, 3, 4])\n",
    "print(type(dev_a))\n",
    "\n",
    "# Can be printed out of the box, though this results in device-host memory copying \n",
    "%time print(dev_a)\n",
    "\n",
    "a = np.arange(10, dtype=int)\n",
    "%time print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cupy also supports all sorts of fancy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOY8Vbyql3tY"
   },
   "outputs": [],
   "source": [
    "# strided with start stop index\n",
    "print(dev_a[1:-1:2])\n",
    "# using list of indices to gather\n",
    "print(dev_a[[0,2,4]])\n",
    "# or with boolean list\n",
    "print(dev_a[dev_a % 3 == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y36fXnRBjRhi"
   },
   "outputs": [],
   "source": [
    "# Easy to transfer arrays between the device and the host\n",
    "\n",
    "a = np.arange(0, 20, 2)\n",
    "dev_a = cp.asarray(a)\n",
    "\n",
    "# The two arrays contain the same elements\n",
    "print(np.allclose(a, dev_a))\n",
    "print(cp.allclose(a, dev_a))\n",
    "# Notice that (many) numpy and cupy functions can accept as arguments both numpy and cupy arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RixjN8LulzlU"
   },
   "outputs": [],
   "source": [
    "# To get an array back to the host is simple:\n",
    "b = cp.asnumpy(dev_a)\n",
    "c = dev_a.get()\n",
    "print(type(b), type(c))\n",
    "\n",
    "# Cupy can  actually operate solely on numpy arrays\n",
    "print(cp.allclose(b, c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hs39nAecC8K"
   },
   "source": [
    "# Supported functions\n",
    "\n",
    "Complete list here:  https://docs.cupy.dev/en/stable/reference/comparison.html\n",
    "\n",
    "Includes Numpy and Scipy routines. \n",
    "\n",
    "CuPy behaves like a drop-in replacement of Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARdY5w5NedfR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "for xp in [np, cp]:\n",
    "    x = xp.arange(10)\n",
    "    W = xp.ones((10, 5))\n",
    "    y = xp.dot(x, W)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuuw3JzILxJW"
   },
   "source": [
    "# Ways to GPU-accelerate a function: 1) Using Numpy/ scipy equivalent operations\n",
    "The easiest way is by using numpy-like array operations and the supported functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfplqvDF0Rc4"
   },
   "outputs": [],
   "source": [
    "# It is trickier to time GPU kernels, because they behave asynchronously w.r.t the host\n",
    "def benchmark(func, args, n_repeat=10, n_warmup=1):\n",
    "    import time \n",
    "    import cupy as cp\n",
    "    start_gpu = cp.cuda.Event()\n",
    "    end_gpu = cp.cuda.Event()\n",
    "    for i in range(n_warmup):\n",
    "        out = func(*args)\n",
    "\n",
    "    start_gpu.record()\n",
    "    start_cpu = time.perf_counter()\n",
    "    for i in range(n_repeat):\n",
    "        out = func(*args)\n",
    "\n",
    "    end_cpu = time.perf_counter()\n",
    "    end_gpu.record()\n",
    "    end_gpu.synchronize()\n",
    "    t_gpu = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "    t_cpu = 1000 * (end_cpu - start_cpu)\n",
    "    print('Average GPU time (ms): ', t_gpu / n_repeat)\n",
    "    print('Average CPU time (ms): ', t_cpu/ n_repeat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "263zDgmfjIzs"
   },
   "outputs": [],
   "source": [
    "# saxpy_trig, just a random compute intensive function\n",
    "def saxpy_trig(x, y, a):\n",
    "    return cp.exp(a * cp.sin(x) + cp.cos(y))\n",
    "\n",
    "dev_x = cp.random.uniform(size=10000000, dtype=np.float32)\n",
    "dev_y = cp.random.uniform(size=10000000, dtype=np.float32)\n",
    "a = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfH393xkuJwk"
   },
   "outputs": [],
   "source": [
    "benchmark(saxpy_trig, (dev_x, dev_y, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6txU784cC-8"
   },
   "source": [
    "# Ways to GPU-accelerate a function: 2) User defined, elementwise or reduction kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_Icg60Xcf_o"
   },
   "source": [
    "## Elementwise kernels\n",
    "When you want to compute an operation that operates on corresponding elements within the input arrays, e.g. `arr_a + arr_b` \n",
    "```python\n",
    "for i in size:\n",
    "    y[i] = F(a[i], b[i], ..., c1, c2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wgnV2f8uoYb"
   },
   "outputs": [],
   "source": [
    "saxpy_trig_elemwise = cp.ElementwiseKernel(\n",
    "    'float32 x, float32 y, float32 a',  # Input types\n",
    "    'float32 z',                        # Output type\n",
    "    'z = exp(a * sin(x) + cos(y))',     # operation\n",
    "    'saxpy_trig_elemwise'               # Kernel name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0YD9a0lvTJN"
   },
   "outputs": [],
   "source": [
    "benchmark(saxpy_trig_elemwise, (dev_x, dev_y, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAm9hy_VciCx"
   },
   "source": [
    "## Reduction kernels\n",
    "When you want to reduce an array to a single element, e.g. `arr_a.sum(), arr_a.max()`.\n",
    "```python\n",
    "y = c\n",
    "for i in size:\n",
    "    y = F(y, a[i], b[i], ..., c1, c2, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aKKCk_T2qYf"
   },
   "outputs": [],
   "source": [
    "saxpy_trig_sum_reduction = cp.ReductionKernel(\n",
    "    'float32 x, float32 y, float32 a',      # input arguments\n",
    "    'float32 z',                            # output arguments\n",
    "    'exp(a * sin(x) + cos(y))',             # map expression\n",
    "    'a + b',                                # Reduce expression\n",
    "    'z = a',                                # post map expression\n",
    "    '0',                                    # identity element\n",
    "    'saxpy_trig_sum_reduction'              # name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjH4od8K3lED"
   },
   "outputs": [],
   "source": [
    "benchmark(saxpy_trig_sum_reduction, (dev_x, dev_y, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzQtOZ16cDBo"
   },
   "source": [
    "# Ways to GPU-accelerate a function: 3) Kernel fusion\n",
    "Fuses together multiple operations in a single kernel launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyfHl3nkO5p0"
   },
   "outputs": [],
   "source": [
    "@cp.fuse(kernel_name='saxpy_trig_fused')\n",
    "def saxpy_trig_fused(x, y, a):\n",
    "    return cp.exp(a * cp.sin(x) + cp.cos(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E403Z3xBPI8w"
   },
   "outputs": [],
   "source": [
    "benchmark(saxpy_trig_fused, (dev_x, dev_y, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically the Kernel fusion method will perform similarly to the Elementwise method.\n",
    "* Notice how Reduction Kernels are much slower compared to Elementwise kernels. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZQsSeuWcDEn"
   },
   "source": [
    "# Interoperability\n",
    "CuPy can be combined with a bunch of other libraries, including Numpy, mpi4py, Pytorch and ... Numba!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZL_-3ZMSaOZ"
   },
   "outputs": [],
   "source": [
    "from numba import vectorize, cuda\n",
    "import cupy as cp\n",
    "import math\n",
    "\n",
    "# We define a numba vectorized ufunc that can be used in the GPU with the target='cuda' argument\n",
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def saxpy_trig_numba_vec(x, y, a):\n",
    "    return math.exp(a * math.sin(x) + math.cos(y))\n",
    "\n",
    "\n",
    "dev_x = cp.random.uniform(size=10000000, dtype=np.float32)\n",
    "dev_y = cp.random.uniform(size=10000000, dtype=np.float32)\n",
    "a = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRUrdfA7Titk"
   },
   "outputs": [],
   "source": [
    "benchmark(saxpy_trig_numba_vec, (dev_x, dev_y, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjSsfOtETCoe"
   },
   "outputs": [],
   "source": [
    "# Or alternatively we can use the @cuda.jit decorator\n",
    "@cuda.jit\n",
    "def saxpy_trig_numba_jit(x, y, a, out):\n",
    "    tid = cuda.grid(1)\n",
    "    if tid < x.shape[0]:\n",
    "        out[tid] = math.exp(a * math.sin(x[tid]) + math.cos(y[tid]))\n",
    "\n",
    "dev_out = cuda.device_array(len(dev_x), dtype=np.float32)\n",
    "\n",
    "block_size = 1024\n",
    "grid_size = int((len(dev_x) + block_size-1)// block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kmc89eqV0Sk"
   },
   "outputs": [],
   "source": [
    "benchmark(saxpy_trig_numba_jit[grid_size, block_size], (dev_x, dev_y, a, dev_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P9eu_N-ZLU1"
   },
   "source": [
    "Notice that in the previous function calls **we passed cupy arrays to Numba CUDA jitted functions, and even a mix of Cupy + Numba cuda arrays**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHbq3cymY3tK"
   },
   "outputs": [],
   "source": [
    "print(type(dev_out))\n",
    "# Zero-copy conversions \n",
    "dev_cp_out = cp.asarray(dev_out)\n",
    "print(type(dev_cp_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K8vLY8Ojtq4"
   },
   "source": [
    "# Other interesting features of CuPy include\n",
    "* JIT decorator to compile unrolled, python-like kernels to CUDA kernels\n",
    "* Can easily import + compile raw CUDA source files. \n",
    "* Acceleration libraries: Some of the most basic operations are accelerated using HPC CUDA backend like CUB and CuTENSOR.\n",
    "* AMD support: CuPy has experimental support for AMD GPUs (ROCm). Increasing set of features supported in AMD GPUs. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ab8fd01a8cec42884b8b2a5d7fb4751c5402d97e9e61d151ed5c6a6352873c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
